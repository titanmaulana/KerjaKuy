{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdfplumber -q\n",
    "# !apt-get install ocrmypdf -q   #installing OCR library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import BigramCollocationFinder\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest\n",
    "from collections import defaultdict\n",
    "import pandas as pd \n",
    "from nltk.collocations import*\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pdfplumber.open(\"/content/drive/MyDrive/test.pdf\") as pdf:\n",
    "#     text=\"\"\n",
    "#     pages = pdf.pages\n",
    "#     for page in pages:\n",
    "#         text += page.extract_text(x_tolerance=2)\n",
    "#         print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import CV and scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CV\n",
    "# f=open(r\"C:\\ikhsan\\UNAIR\\MBKM\\Bangkit\\Profile.pdf\",'r', errors = 'ignore')\n",
    "# text=f.read()\n",
    "\n",
    "# import pdfminer\n",
    "# from pdfminer.high_level import extract_text\n",
    "\n",
    "# # Read PDF file and extract text\n",
    "# def read_pdf(file_path):\n",
    "#     text = extract_text(file_path)\n",
    "#     # print(text)\n",
    "#     return text\n",
    "\n",
    "# # Preprocess text\n",
    "# def preprocess_text(text):\n",
    "#     # Implement text preprocessing steps as per your requirements\n",
    "#     # For example, remove special characters, convert to lowercase, etc.\n",
    "#     # print(text)\n",
    "#     return text\n",
    "\n",
    "# resume_path = r\"C:\\ikhsan\\UNAIR\\Template CV ATS-Friendly Glints\\CV panjang.pdf\"\n",
    "# resume_text = read_pdf(resume_path)\n",
    "# text = preprocess_text(resume_text)\n",
    "\n",
    "with pdfplumber.open(r\"C:\\ikhsan\\UNAIR\\Template CV ATS-Friendly Glints\\CV panjang.pdf\") as pdf:\n",
    "    text=\"\"\n",
    "    pages = pdf.pages\n",
    "    for page in pages:\n",
    "        text += page.extract_text(x_tolerance=2)\n",
    "        print(text)\n",
    "\n",
    "# Locations\n",
    "# f1=open(r\"data\\Location.txt\",'r', errors = 'ignore')\n",
    "# text1=f1.read()\n",
    "# locations = word_tokenize(text1.replace(\"\\n\", \" \"))\n",
    "\n",
    "# Additional stopwords\n",
    "f2=open(r\"data\\stopwords.txt\",'r', errors = 'ignore')\n",
    "text2=f2.read()\n",
    "stopwords_additional = word_tokenize(text2.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp(x):\n",
    "    word_sent = word_tokenize(x.lower().replace(\"\\n\",\"\"))\n",
    "    _stopwords = set(stopwords.words('english', 'indonesian') + list(punctuation)+list(\"●\")+list('–')+list('’')+stopwords_additional)\n",
    "    word_sent=[word for word in word_sent if word not in _stopwords]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    NLP_Processed_CV = [lemmatizer.lemmatize(word) for word in word_tokenize(\" \".join(word_sent))]\n",
    "#     return \" \".join(NLP_Processed_CV)\n",
    "    return NLP_Processed_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_word(x):\n",
    "    finder = BigramCollocationFinder.from_words(x)\n",
    "    keyword = []\n",
    "    keywords_CV = []\n",
    "    for i in sorted(finder.ngram_fd.items()):\n",
    "    # if a double word appears more than once, then print it out.\n",
    "        if i[1] > 1:\n",
    "            print(i)\n",
    "            keyword.append(i[0])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # print(\"************************\")\n",
    "    for j in keyword:\n",
    "    #     print(\" \".join(j))\n",
    "        keywords_CV.append(\" \".join(j))\n",
    "    return keywords_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_Processed_CV = nlp(text)\n",
    "# NLP_Processed_CV=' '.join(NLP_Processed_CV)\n",
    "NLP_Processed_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_word = get_key_word(NLP_Processed_CV)\n",
    "key_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scraped data\n",
    "df = pd.read_excel(r\"data\\data website indeed Jakarta (30 May 2023) last.xlsx\", \"Part-Time\")\n",
    "index = []\n",
    "for i in range (0, len(df)):\n",
    "    index.append(i)\n",
    "df.insert(0,'JobID', index)\n",
    "df['All'] = df['Posisi'] + ' ' + df['Perusahaan '] + ' ' + df['Lokasi'] + ' ' + df['Tipe Pekerjaan'] + ' ' + df['Kualifikasi'] + ' ' + df['Deskripsi Pekerjaan ']\n",
    "df.rename(columns = {'Perusahaan ':'Perusahaan', 'Deskripsi Pekerjaan ': 'Deskripsi Pekerjaan'}, inplace = True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(NLP_Processed_CV)\n",
    "# print(key_word)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(decode_error='replace', encoding='utf-8', stop_words=stopwords.words('english', 'indonesian'))\n",
    "tfidf_jobid = tfidf_vectorizer.fit_transform(df['All'].apply(lambda x: np.str_(x)))\n",
    "# print(tfidf_jobid)\n",
    "user_tfidf = tfidf_vectorizer.transform(df['All'].apply(lambda x: np.str_(x)))\n",
    "# print(user_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity_tfidf = map(lambda x: cosine_similarity(user_tfidf,x),tfidf_jobid)\n",
    "outputJob = list(cos_similarity_tfidf)\n",
    "outputJob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put keyword/CV into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "# append columns to an empty DataFrame\n",
    "df2['Posisi'] = [\"I\"]\n",
    "#df2['job highlights'] = [\"I\"]\n",
    "df2['Deskripsi Pekerjaan'] = [\"I\"]\n",
    "# df2['company overview'] = [\"I\"]\n",
    "df2['Perusahaan'] = [\"I\"]\n",
    "\n",
    "# Compare with the key words from CV only\n",
    "df2['All'] = \" \".join(key_word)\n",
    "df2.head()\n",
    "# Compare with the entire CV\n",
    "# df2['All'] = \" \".join(NLP_Processed_CV)\n",
    "# df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create get_recommendation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation(top, df_all, scores):\n",
    "    recommendation = pd.DataFrame(columns = ['JobID',  'Posisi', 'Perusahaan', 'Deskripsi Pekerjaan','Score'])\n",
    "    count = 0\n",
    "    for i in top:\n",
    "#         recommendation.at[count, 'ApplicantID'] = u\n",
    "        recommendation.at[count, 'JobID'] = df.index[i]\n",
    "        recommendation.at[count, 'Posisi'] = df['Posisi'][i]\n",
    "        recommendation.at[count, 'Perusahaan'] = df['Perusahaan'][i]\n",
    "#         recommendation.at[count, 'location'] = df['location'][i]\n",
    "        recommendation.at[count, 'Deskripsi Pekerjaan'] = df['Deskripsi Pekerjaan'][i]\n",
    "        recommendation.at[count, 'Score'] =  scores[count]\n",
    "        count += 1\n",
    "    return recommendation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def TFIDF(scraped_data, cv):\n",
    "    tfidf_vectorizer = TfidfVectorizer(decode_error='replace', encoding='utf-8', stop_words=stopwords.words('english', 'indonesian'))\n",
    "\n",
    "    # TF-IDF Scraped data\n",
    "    tfidf_jobid = tfidf_vectorizer.fit_transform(scraped_data.apply(lambda x: np.str_(x)))\n",
    "    # tfidf_jobid = tfidf_vectorizer.fit_transform(scraped_data)\n",
    "\n",
    "    # TF-IDF CV\n",
    "    user_tfidf = tfidf_vectorizer.transform(cv.apply(lambda x: np.str_(x)))\n",
    "    # user_tfidf = tfidf_vectorizer.transform(cv)\n",
    "\n",
    "    # Using cosine_similarity on (Scraped data) & (CV)\n",
    "    cos_similarity_tfidf = map(lambda x: cosine_similarity(user_tfidf,x),tfidf_jobid)\n",
    "\n",
    "    output2 = list(cos_similarity_tfidf)\n",
    "    return output2\n",
    "\n",
    "outputTF = TFIDF(df['All'], df2['All'])\n",
    "outputTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = sorted(range(len(outputTF)), key=lambda i: outputTF[i], reverse=True)[:100]\n",
    "list_scores = [outputTF[i][0][0] for i in top]\n",
    "TF = get_recommendation(top, df, list_scores)\n",
    "TF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# print(tfidf_jobid)\n",
    "user_tfidf = tfidf_vectorizer.transform(df['All'].apply(lambda user_tfidf: np.str_(user_tfidf)))\n",
    "\n",
    "def count_vectorize(scraped_data, cv):\n",
    "    # CountV the scraped data\n",
    "    count_vectorizer = CountVectorizer(decode_error='replace', encoding='utf-8', stop_words=stopwords.words('english', 'indonesian'))\n",
    "    # count_jobid = count_vectorizer.fit_transform(scraped_data) #fitting and transforming the vector\n",
    "    count_jobid = count_vectorizer.fit_transform(scraped_data.apply(lambda x: np.str_(x)))\n",
    "\n",
    "    # CountV the cv\n",
    "    # user_count = count_vectorizer.transform(cv)\n",
    "    user_count = count_vectorizer.transform(cv.apply(lambda x: np.str_(x)))\n",
    "    cos_similarity_countv = map(lambda x: cosine_similarity(user_count, x),count_jobid)\n",
    "    output3 = list(cos_similarity_countv)\n",
    "    return output3\n",
    "\n",
    "outputCV = count_vectorize(df['All'], df2['All'])\n",
    "outputCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "count_vectorizer = CountVectorizer(decode_error='replace', encoding='utf-8', stop_words=stopwords.words('english', 'indonesian'))\n",
    "count_jobid = count_vectorizer.fit_transform(df['All'].apply(lambda x: np.str_(x))) #fitting and transforming the vector\n",
    "print(count_jobid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_count = count_vectorizer.transform(df2['All'].apply(lambda x: np.str_(x)))\n",
    "print(user_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity_countv = map(lambda x: cosine_similarity(user_count, x),count_jobid)\n",
    "outputCV = list(cos_similarity_countv)\n",
    "outputCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = sorted(range(len(outputCV)), key=lambda i: outputCV[i], reverse=True)[:100]\n",
    "list_scores = [outputCV[i][0][0] for i in top]\n",
    "cv = get_recommendation(top, df, list_scores)\n",
    "cv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def KNN(scraped_data, cv):\n",
    "    tfidf_vectorizer = TfidfVectorizer(decode_error='replace', encoding='utf-8', stop_words=stopwords.words('english', 'indonesian'))\n",
    "    n_neighbors = 100\n",
    "    KNN = NearestNeighbors(n_neighbors=10)\n",
    "    KNN.fit(tfidf_vectorizer.fit_transform(scraped_data.apply(lambda x: np.str_(x))))\n",
    "#     NNs = KNN.kneighbors(tfidf_vectorizer.transform(cv), return_distance=True)\n",
    "    NNs = KNN.kneighbors(tfidf_vectorizer.transform(cv.apply(lambda x: np.str_(x))))\n",
    "    top = NNs[1][0][0:]\n",
    "    print(top)\n",
    "    index_score = NNs[0][0][0:]\n",
    "\n",
    "    knn = get_recommendation(top, df, index_score)\n",
    "    return knn\n",
    "\n",
    "knn = KNN(df['All'], df2['All'])\n",
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn.to_csv('knn.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined TFIDF, CV and KNN result together, to make a dataframe \"final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge1 = knn[['JobID','title', 'score']].merge(TF[['JobID','score']], on= \"JobID\")\n",
    "# final = merge1.merge(cv[['JobID','score']], on = \"JobID\")\n",
    "# final = final.rename(columns={\"score_x\": \"KNN\", \"score_y\": \"TF-IDF\",\"score\": \"CV\"})\n",
    "# final.head()\n",
    "\n",
    "merge1 = knn[['JobID','Posisi', 'Score']].merge(TF[['JobID','Score']], on= \"JobID\")\n",
    "final = merge1.merge(cv[['JobID','Score']], on = \"JobID\")\n",
    "final = final.rename(columns={\"Score_x\": \"KNN\", \"Score_y\": \"TF-IDF\",\"Score\": \"CV\"})\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverseScoring(knn, high, 'score').to_csv('test.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale and assign weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale it\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "slr = MinMaxScaler()\n",
    "final[[\"KNN\", \"TF-IDF\", 'CV']] = slr.fit_transform(final[[\"KNN\", \"TF-IDF\", 'CV']])\n",
    "\n",
    "# Multiply by weights\n",
    "final['KNN'] = (1-final['KNN'])/3\n",
    "final['TF-IDF'] = final['TF-IDF']/3\n",
    "final['CV'] = final['CV']/3\n",
    "final['Final'] = final['KNN']+final['TF-IDF']+final['CV']\n",
    "final.sort_values(by=\"Final\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final.to_csv('final.csv', index=False)\n",
    "# final2 = final.sort_values(by=\"Final\", ascending=False).copy()\n",
    "# final2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final2.merge(df, on=\"JobID\")\n",
    "final_all = df.merge(final, on=\"JobID\")\n",
    "final_job = final_all.sort_values(by=\"Final\", ascending=False).copy()\n",
    "final_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(TF, open('artifacts/TF.pkl', 'wb'))\n",
    "pickle.dump(cv, open('artifacts/cv.pkl', 'wb'))\n",
    "pickle.dump(knn, open('artifacts/knn.pkl', 'wb'))\n",
    "pickle.dump(final, open('artifacts/final.pkl', 'wb'))\n",
    "pickle.dump(final_job, open('artifacts/final_job.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
